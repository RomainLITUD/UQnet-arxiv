{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heatmap_model.interaction_model import UQnet,TrajModel\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torchvision.datasets as dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import datetime\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "from heatmap_model.utils import *\n",
    "from heatmap_model.train import *\n",
    "from heatmap_model.interaction_dataset import *\n",
    "from heatmap_model.losses import *\n",
    "from config import *\n",
    "\n",
    "from absl import logging\n",
    "logging._warn_preinit_stderr = 0\n",
    "logging.warning('Worrying Stuff')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook can reproduce the results about uncertainty quantification in the paper\n",
    "\n",
    "- we choose the resolution = 0.5m in inference. Any resolution works, we can directly use the pre-trained models.\n",
    "- Here we only show the MR minimization sampling strategy, which is the result on the INTERPRET leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralist['resolution'] = 0.5\n",
    "paralist['encoder_attention_size'] = 128\n",
    "paralist['use_sem'] = False\n",
    "paralist['mode'] = 'lanescore'\n",
    "paralist['prob_mode'] = 'ce'\n",
    "paralist['inference'] = True\n",
    "model = UQnet(paralist, test=True, drivable=False).to(device) # set test=True here\n",
    "testset = InteractionDataset(['val'], 'val', paralist, mode=paralist['mode'], filters=False) # for validation\n",
    "#testset = InteractionDataset(['test'], 'test', paralist, mode=paralist['mode'], filters=False) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For predicted best 6 positions and uncertainty quantification, run the following block\n",
    "- <code>'val'</code> can be changed to <code>'val_all'</code> or <code>'test'</code> as needed.\n",
    "- <code>Yp</code> is predicted results, <code>Ua</code> is aleatoruc uncertainty, <code>Um</code> is epistemic uncertainty, <code>Y</code> is the ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yp, Ua, Um, Y = inference_model(model, './interactionDE/', 'val', 7, testset, paralist, k=6, \n",
    "                                test=False, return_heatmap=False, mode='lanescore', batch=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you also want to visualize the heatmap results, run the following block (much slower)\n",
    "- <code>H</code> is the heatmap, the other outputs are not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, T, Nm, Na, Yp, Ua, Um, H, Y = inference_model(model, './interactionDE/', 'val', 7, testset, paralist, \n",
    "                                              k=6, test=False, return_heatmap=True, mode=paralist['mode'], batch=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of accuracy evaluation\n",
    "- <code>r</code> is radius and <code>sh</code> is the best <code>sh</code>-th prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDE = ComputeError(Yp,Y, r=2, sh=6)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
